{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción al manejo de datos geográficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase 2: manejo de APIs geográficas y estrategias de visualización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lo largo del notebook anterior vimos algunas cuestiones generales del manejo de datos espaciales. Los tipos de geometrías y su implementación en `shapely`, los métodos propios a cada una de ellas, manipulación de geodataframes con `geopandas` y, también, algún avance de cómo visualizar sobre un mapa las transformaciones que ibamos efectuando sobre nuestros datos.\n",
    "\n",
    "Ahora, intentaremos hacer un repaso de las librerías más utilizadas para el ploteo de información geográfica. Estas herramientas son de gran utilidad ya que, combinándolas con los recursos que venimos revisado, amplían las posibilidades que brinda un SIG de escritorio clásico como QGIS o ARCGIS.\n",
    "\n",
    "Para usarlas, construiremos antes el problema que queremos trabajar espacialmente. Para ello, partiremos de una circunstancia bastante frecuente en el mundo de los datos geográficos: la ausencia de coordenadas o de algún otro atributo que permita su la rápida visualización sobre un layer. Este problema lo vamos a resolver revisando algunas APIs de normalización y enriquecimiento de unidades geográficas. \n",
    "\n",
    "A partir de un caso concreto, introduciremos dos APIs diferentes. Y con cada una de ellas, daremos respuesta al mismo problema. Reconstruiremos primero la dimensión espacial de nuestros datos y compararemos resultados apelando a distintas librerías de visualización. Comencemos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una situación bastante común cuando trabajamos con datos que tienen una representación espacial, es que esta no esté disponible como columna o Serie de un DataFrame de pandas. En otras palabras, que no dispongamos de la latitud y la longitud de los datos que buscamos representar espacialmente. Pongamos un ejemplo a partir de un caso de uso real y veamos cómo solucionarlo. \n",
    "\n",
    "Supongamos que nuevamente necesitamos trabajar con edificios. Pero en esta oportunidad, no con desarrollos inmobiliarios sino con edificios que han sido catalogados como patrimonio histórico de la ciudad. \n",
    "\n",
    "A continuación veremos un [listado de fachadas](https://data.buenosaires.gob.ar/dataset/fachadas) con certificado de conservación. Este dataframe lo bajamos del portal de datos del Gobierno de la Ciudad de Buenos Aires, comúnmente conocido como [data buenos aires](https://data.buenosaires.gob.ar/dataset) - un sitio de consulta de datasets de bastante utilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FACHADAS](imagenes/fachadas.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Damos un primer vistazo a nuestro DataFrame\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('data/fachadas.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar, este listado cuenta con la dirección física de cada una de las fachadas. Pero si quisiéramos representarlas en un mapa necesitaríamos de una serie de atributos que no están provistos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Servicio de Normalización de Datos Geográficos (MM)\n",
    "\n",
    "En el notebook anterior vimos qué es un geocodificador y cómo se puede obtener las coordenadas geográficas de un punto específico a partir de la API de Google. Ahora vamos a ampliar un poco el espectro e introduciremos otra herramienta del estilo que desarrolló el Gobierno Nacional. Esta es la [API del Servicio de Normalización de Datos Geográficos de Argentina](https://datosgobar.github.io/georef-ar-api/), una herramienta muy útil para normalizar entidades territoriales, enriquecerlas u obtener información como sus coordenadas.\n",
    "\n",
    "Esta herramienta nos será de mucha utilidad para el problema que necesitamos abordar: `obtener la latitud y longitud de un punto a partir de un string de direcciones físicas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero instanciemos nuestro dataframe y construyamos nuestro string de direcciones\n",
    "fachadas = pd.read_csv('data/fachadas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como la altura es un valor numérico,\n",
    "fachadas.calle_altura.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a reemplazar los NaN por cero\n",
    "fachadas.calle_altura.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y ver cuántos son. Como es bajo el número vamos a suponer que los faltantes corresponden a la altura 0.\n",
    "len(fachadas.loc[fachadas.calle_altura == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y ahora convertimos las alturas en integer,\n",
    "fachadas.calle_altura = fachadas.calle_altura.map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para luego guardarlas como string.\n",
    "fachadas.calle_altura = fachadas.calle_altura.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armamos un único string\n",
    "fachadas['direccion'] = fachadas['calle_nombre'] + ' ' + fachadas['calle_altura']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que ya tenemos nuestro string listo, veamos un primer ejemplo para enriquecer nuestras entidades geogŕaficas revisando algunos de los [ejemplos en python](https://datosgobar.github.io/georef-ar-api/python-usage/).\n",
    "\n",
    "Lo que vamos a hacer es tomar el caso de normalización de varias entidades que se lleva a cabo para consultas provinciales. Por medio de la función `similar_bulk` se construirá un diccionario con el listado de direcciones. Se utilizará el parámetro `endpoint`, con el nombre de lo que será la llave con la que se accederá a una lista de diccionarios. Cada uno de esos diccionarios contará con la key `dirección` que dará acceso al `str` que construimos más arriba con el nombre de la dirección. Es decir, la lista de direcciones que pasamos como segundo parámetro. \n",
    "\n",
    "También vamos a agregar en el cuerpo de la función una información adicional: el `departamento` al que pertenece nuestra entidad. Así es como se guarda la data que será consultada en el endpoint a través del request. Las keys dirección y departamento son las que darán acceso a la base de datos de donde se consumirá la información adicional que nos traeremos de vuelta. \n",
    "\n",
    "Para esto nos fijamos cómo se construye la API BASE para direcciones en los [ejemplos de uso](https://datosgobar.github.io/georef-ar-api/quick-start/). Así, podemos ver que el formato de la consulta es el siguiente: `https://apis.datos.gob.ar/georef/api/direcciones?departamento=merlo&direccion=Florida al 2950`. En nuestro caso, como estamos en la Ciudad de Buenos Aires nuestro departamento serán las Comunas. Al tener 15 posibilidades, podría ser que debiéramos haber completado con el `str` de las `15 Comunas`. Para lo cual, deberíamos haber incorporado un parámetro adicional en la función `get_similar_bulk`. Algo que hiciera masomenos lo siguiente:\n",
    "\n",
    "```\n",
    "def get_similar_bulk(endpoint, nombres, departamento):\n",
    "    (...)\n",
    "    return parsed_results\n",
    "```\n",
    "\n",
    "Pero eso hubiese llevado a la tarea de introducir la consulta en un loop que iterara sobre el rango 1 a 15 y testeara las quince posibilidades diferentes para cada dirección - dado que por la información con la que cuenta nuestra base, tampoco sabemos a qué comuna pertenece cada una de ellas-. Algo verdaderamente costoso en consumo de memoria. Pero por suerte la API resuelve muy bien este caso de uso. Con sólo utilizar la palabra `Comuna` para el parámetro `departamento` que se consumirá en el endpoint será suficiente para traer la consulta sin error.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importemos los módulos necesarios para el uso de la API\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos un string con la url base que vamos a usar para construir las consultas\n",
    "API_BASE_URL = \"https://apis.datos.gob.ar/georef/api/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la función\n",
    "def get_similar_bulk(endpoint, nombres):\n",
    "    '''\n",
    "    Normaliza una lista de nombres de alguna de \n",
    "    las entidades geográficas.\n",
    "    ...\n",
    "    Argumentos:\n",
    "        endpoint(str): String de texto.\n",
    "        nombres (iter): Objeto iterable (e.g: list, Series)\n",
    "    Devuelve:\n",
    "        list: diccionarios con el resultado de la consulta \n",
    "    '''\n",
    "\n",
    "    # realiza consulta a la API\n",
    "    data = {\n",
    "        endpoint: [\n",
    "            {\"direccion\": nombre,\n",
    "             \"departamento\":'Comuna',\n",
    "             \"max\":5} for nombre in nombres\n",
    "    ]}\n",
    "    url = API_BASE_URL + endpoint\n",
    "    results = requests.post(\n",
    "        url, json=data, headers={\"Content-Type\": \"application/json\"}\n",
    "    ).json()\n",
    "    \n",
    "    try:\n",
    "        parsed_results = [\n",
    "            single_result[endpoint][0] if single_result[endpoint] else {'Sin dato'}\n",
    "            for single_result in results[\"resultados\"]\n",
    "        ]\n",
    "        print('Consulta realizada: %r resultados obtenidos' % len(parsed_results))\n",
    "\n",
    "    except:\n",
    "        print('Excediste el limite de consultas')\n",
    "        parsed_results = {'Sin dato'}\n",
    "\n",
    "    return parsed_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si comparamos la función con la del ejemplo provisto en la web de la API se puede identificar que agregamos un `try`/`except` al final. Antes de usar este tipo de servicios, es recomendable leer las [condiciones de uso](https://datosgobar.github.io/georef-ar-api/terms/). En el caso de esta API, las [consultas por lote](https://datosgobar.github.io/georef-ar-api/bulk/) tienen límites máximos. Veamos qué pasa si no los respetamos...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este es el total de direcciones que deberíamos consultar\n",
    "len(fachadas.direccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres = fachadas.direccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos ver que en algún momento de la consulta, la API dejó de responder y pasamos al bloque except.\n",
    "direcciones = get_similar_bulk(\"direcciones\", nombres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es porque la cantidad de consultas en una misma petición no debe superar las `1000`. Nosotros definimos 5 como `max`, con lo cual si ahora vamos por 999 no deberían tener ningún problema en hacer la consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consultas(totales):\n",
    "    '''\n",
    "    Aplica get_similar_bulk sobre\n",
    "    la serie de direcciones de un df\n",
    "    ...\n",
    "    Argumentos:\n",
    "        totales (int): límite superior del intervalo.\n",
    "    Devuelve:\n",
    "        list: diccionarios con el resultado de cada consulta \n",
    "    '''\n",
    "    intervalos = np.arange(0, totales, 999).tolist()\n",
    "    intervalos.append(totales)\n",
    "    \n",
    "    listado = []\n",
    "    for i in range(len(intervalos)):\n",
    "        try:\n",
    "            print(\"Iterando de %r a %r casos\" % (intervalos[i], intervalos[i+1]))\n",
    "            I = intervalos[i]\n",
    "            F = intervalos[i+1]\n",
    "        except:\n",
    "            print(\"Consulta terminada\")\n",
    "            break\n",
    "\n",
    "        nombres = fachadas.direccion.iloc[I:F]\n",
    "        nombres.fillna('Sin dato', inplace=True)\n",
    "        direcciones = get_similar_bulk(\"direcciones\", nombres)\n",
    "        listado.extend(direcciones)\n",
    "        \n",
    "    return listado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos get_similar_bulk sobre todo el listado de direcciones de nuestro df\n",
    "para_enriquecer = consultas(len(fachadas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tenemos la misma cantidad de casos en nuestra consulta\n",
    "len(para_enriquecer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y en nuestro dataframe original\n",
    "len(fachadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y estas son las keys que tenemos en cada consulta. Ponemos la primera sólo para verlas.\n",
    "para_enriquecer[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enriquece(consultas, atributo):\n",
    "    '''\n",
    "    Accede al item de cada key de \n",
    "    obtenido en la conuslta.\n",
    "    ...\n",
    "    Argumentos:\n",
    "        consultas (list): lista de diccionarios.\n",
    "        atributo (str): nombre de la key\n",
    "    Devuelve:\n",
    "        list: string con el atributo consultado \n",
    "    '''\n",
    "    resultado = []\n",
    "    for i in range(len(consultas)):\n",
    "        if type(consultas[i]) is dict:\n",
    "            if atributo in consultas[i].keys():\n",
    "                resultado.append(consultas[i][atributo])\n",
    "        else:\n",
    "            resultado.append('Sin resultado')\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos traemos la nomenclatura y vemos que seguimos teniendo la misma cantidad de casos. Es un buen signo!\n",
    "len(enriquece(para_enriquecer, 'nomenclatura'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora sumamos esta info a nuestro dataframe\n",
    "fachadas['nomenclatura'] = enriquece(para_enriquecer, 'nomenclatura')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fachadas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos una columna de nomenclatura con toda la dirección completa. Cómo sería enriquecer nuestro dataframe con el resto de los atributos? Por ejemplo, con la información del departamento..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como se puede ver, algunos casos son listas de diccionarios donde cada key es un potencial atributo...\n",
    "pd.Series(enriquece(para_enriquecer, 'departamento')).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para estos casos, vamos a construir una funcion que nos permita ir recolectando los atributos de interés. Por ejemplo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devuelve_subatributo(atributo, subatributo):\n",
    "    '''\n",
    "    Accede a los items de las listadas almacenadas\n",
    "    dentro del diccionario.\n",
    "    ...\n",
    "    Argumentos:\n",
    "        atributo(str): string de texto con el nombre de la key.\n",
    "        subatributo (list): lista de string con el nombre de\n",
    "                            de los subatributos.\n",
    "    Devuelve:\n",
    "        item: elemento de cada key ordenada en la lista. \n",
    "    '''\n",
    "    atributos = enriquece(para_enriquecer, atributo)\n",
    "    return [atributos[i][subatributo] if type(atributos[i]) \n",
    "            is dict else 'Sin dato' for i in range(len(atributos))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos los parametros de la función\n",
    "targets = {'departamento':['id','nombre'],\n",
    "           'ubicacion':['lat','lon'],\n",
    "           'localidad_censal':['id','nombre']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y la aplcamos en un for loop.\n",
    "for k,v in targets.items():\n",
    "    print(k, v[0], v[1])\n",
    "    fachadas[v[0]],fachadas[v[1]] = devuelve_subatributo(k,v[0]), devuelve_subatributo(k,v[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ahora, qué problema ven con esto? Se guardan las columnas como nosotros queremos o hay algún efecto no deseado?** \n",
    "\n",
    "**Podrían identificar cuál es el problema?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solución\n",
    "for k,v in targets.items():\n",
    "    print(k, v[0], v[1])\n",
    "    if k == 'departamento':\n",
    "        fachadas[v[0]+'dto'],fachadas[v[1]+'dto'] = devuelve_subatributo(k,v[0]), devuelve_subatributo(k,v[1])\n",
    "    else:\n",
    "        fachadas[v[0]],fachadas[v[1]] = devuelve_subatributo(k,v[0]), devuelve_subatributo(k,v[1])     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efetivamente, como dos de nuestros atributos tenían el mismo nombre, el último pisaba el primero y por eso sólo veíamos las columnas de id y nombre del último caso iterado. Veamos ahora nuestro dataframe enriquecido..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fachadas.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportemoslo para no tener que correr todo de nuevo\n",
    "#fachadas.to_csv('fachadas_sndg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizador AMBA (USIG-GCBA)\n",
    "\n",
    "Otro servicio similar es el [normalizador de direcciones](https://pypi.org/project/usig-normalizador-amba/) del Gobierno de la Ciudad de Buenos Aires. Antes de comenzar a utilizarlo, vamos a hacer algunas aclaraciones. Revisando el [release history](https://pypi.org/project/usig-normalizador-amba/#history), constatamos que la [versión más reciente](https://github.com/usig/normalizador-amba) debería estar en `1.3.0`. Sin embargo, una versión más reciente (`2.1.2`) se encuentra disponible en [sitios oficiales](http://servicios.usig.buenosaires.gob.ar/normalizar). Aparentemente esta aún no se encuentra disponible en `pip`. Por lo tanto, iremos recorriendo algunos ejemplos de la última release de Git y revisando el instructivo disponible en el último hipervínculo compartido más arriba. Prosigamos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalemos desde pip si es que aún no lo hiciste\n",
    "#!pip install usig-normalizador-amba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el normalizador\n",
    "from usig_normalizador_amba import NormalizadorAMBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algo interesante de esta API (que no vimos en la anterior) es que permite normalizar una dirección parseando strings de texto y eligiendo el distrito objetivo (esto siempre dentro del Área Metropolitana de Buenos Aires). En la función que armamos a continuación vemos dos formas diferentes de instanciar el normalizador. Introduciendo un `Boolean` parameter vamos a elegir cuál de los dos usar. Ambos devuelven la misma información pero nos sirven para resolver situaciones diferentes. Si nuestro input ya se encuentra formateado como dirección, es decir, no se encuentra con formatos del estilo `calle 1, esquina calle 2` - por mencionar sólo un ejemplo - utilizaremos el normalizador simplemente. Mientras que en el segundo caso lo instanciaremos como `buscarDireccion`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliza(direccion, parsea_texto):\n",
    "    '''\n",
    "    Normaliza una dirección geográficas.\n",
    "    ...\n",
    "    Argumentos:\n",
    "        dirección (str): String de texto.\n",
    "        parsea_texto (bool): Buscar dirección en texto crudo.\n",
    "    Devuelve:\n",
    "        dict: nombre, altura y coords de la dirección. \n",
    "    '''\n",
    "    nd = NormalizadorAMBA(include_list=['caba'])\n",
    "    string = u'{}'.format(direccion)\n",
    "    \n",
    "    try:\n",
    "        if parsea_texto:\n",
    "            res = nd.buscarDireccion(string)\n",
    "            for r in res:\n",
    "                n = res[0][0]['direcciones'][0].calle.nombre\n",
    "                a = res[0][0]['direcciones'][0].altura\n",
    "                c = res[0][0]['direcciones'][0].coordenadas\n",
    "                \n",
    "            return {'nombre': n, \n",
    "                    'altura': a,\n",
    "                    'coord': c}\n",
    "        \n",
    "        else:\n",
    "            res = nd.normalizar(string)\n",
    "        \n",
    "            for r in res:\n",
    "                n = r.calle.nombre\n",
    "                a = r.altura\n",
    "                c = r.coordenadas\n",
    "\n",
    "            return {'nombre': n, \n",
    "                    'altura': a,\n",
    "                    'coord': c}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apliquemos esta función sobre las dos primeras direcciones de nuestro dataframe\n",
    "fachadas.direccion[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como las direcciones están en una serie de pandas podemos usar la función anónima lambda\n",
    "fachadas.direccion[0:2].apply(lambda x: normaliza(x, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probemos cómo hubiese sido si nuestras direcciones se encontraban en un formato más crudo\n",
    "pd.Series(['Julián Álvarez al 2531', 'emilio lamarca 1014 (y galicia)']).apply(lambda x: normaliza(x,'Texto'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aparentemente, en nuestro dataframe las direcciones ya se encontraban normalizadas. Esto lo podemos constatar viendo cómo se normalizan los casos en formato más crudo. Ahora bien, parece haber un problema. Y este es que en ninguna de las dos oportunidades nos duevle coordenadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "print(fachadas.direccion[0:2].apply(lambda x: normaliza(x, False))[0]['coord'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "print(pd.Series(['Julián Álvarez al 2531', \n",
    "                 'emilio lamarca 1014 (y galicia)']).apply(lambda x: normaliza(x,'Texto'))[0]['coord'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de continuar, evaluemos un poco la performance del normalizador. Qué tiempo nos llevaría correrlo sobre toda nuestra serie de direcciones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# calculamos tiempo de ejecución\n",
    "inicio = time.time()\n",
    "partes = fachadas.direccion[0:1000].apply(lambda x: normaliza(x, 'Texto'))\n",
    "fin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tiempo de consulta para 1000 casos: %r segundos' % (fin-inicio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eso quiere decir que haberlo aplicado a toda nuestra serie hubiese llevado...\n",
    "tiempo_total = (((fin - inicio)*26000)/1000)/60\n",
    "print('Tiempo de consulta para la totalidad de casos: %r minutos' % tiempo_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modularizando el normalizador..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que hasta ahora no hemos conseguido las coordenadas geográficas con el normalizador, vamos a combinar algunos pasos que encapsularemos en una función única. Como se ejemplifica en esta [consulta](http://servicios.usig.buenosaires.gob.ar/normalizar/?direccion=juli%C3%A1n%20alvarez%202351,%20caba), la url `http://servicios.usig.buenosaires.gob.ar/normalizar` puede ser completada con una `/direccion=(...)` que devuelve la metadata de la misma. \n",
    "\n",
    "Lo que vamos a hacer a continuación es disponer de la librería `urllib` para para traernos ese resultado. Para ello, mostraremos dos caminos alternativos pero con el mismo punto de llegada. Uno largo y otro corto. En el primero veremos cómo trabajar con recursos en su mayoría nativos de python. En el segundo, apelando a métodos específicamente diseñados para lidiar con la situación que debemos sortear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EL CAMINO LARGO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema que tenemos que resolver es básicamente la adaptación del formato de nuestros strings de direcciones. Como necesitamos que estos nombres se ubiquen en el contexto de una url, es importante que los mismos sean legibles en código [ASCII](https://www.w3schools.com/charsets/ref_html_ascii.asp) para que la consulta se efectúe correctamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" width=\"500\" height=\"125\" src=\"imagenes/camino_largo.jpeg\" style=\"float: left; padding: 0 15px\">\n",
    "Lo que especificamente necesitamos hacer es enviar un request a una página web utilizando un URL. Las URLs pueden enviarse a Internet solamente con un formato ASCII válido. Dado que nuestros string de direcciones van a contener caracteres que están por fuera de este encoding, debemos garantizar que los caracteres no seguros sean transformados en el contexto mismo de la URL. \n",
    "\n",
    "Casos como espacios entre palabras o tildes deberán ser reemplazados por su equivalente en ASCII. Tal es así que en esta seccion mostraremos cómo construir la dirección que debemos incluir en la consulta de una manera más pythonesca, apelando a métodos que son aplicables al string de texto que iremos modificando para darle un encoding compatible a la url. Realizaremos algunas [transormaciones](https://www.w3schools.com/tags/ref_urlencode.ASP) que harán que la dirección que enviemos sea legible en el request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comencemos entonces por crear el grupo de funciones con las que modificaremos nuestra serie de direcciones. Lo que haremos por medio de estas es, en primer lugar, separar el nombre de la dirección de su altura y completar los espacios entre strings con el código ASCII que los reemplaza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separa_partes(string):\n",
    "    '''\n",
    "    Divide caracteres alfa numéricos dentro\n",
    "    de un string.\n",
    "    ...\n",
    "    Argumentos:\n",
    "        string(str): String de texto.\n",
    "    Devuelve:\n",
    "        dict: calle y altura. \n",
    "    '''\n",
    "    calle, altura = '', ''\n",
    "\n",
    "    for i in string:\n",
    "        if i.isdigit():\n",
    "            altura+=str(i)\n",
    "        else:\n",
    "            calle+=str(i)\n",
    "            \n",
    "    return {'nombre': calle,\n",
    "            'altura': altura}        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos el resultado de nuestra función\n",
    "separa_partes(fachadas.direccion[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unifica_direccion(calle, altura):\n",
    "    '''\n",
    "    Unifica caracteres alfanuméricos\n",
    "    reemplazando espacios por %20\n",
    "    ...\n",
    "    Argumentos:\n",
    "        calle(str): string de letras.\n",
    "        altura(str): string de numeros\n",
    "    Devuelve:\n",
    "        str: direccion unificada. \n",
    "    '''\n",
    "    nombre_url = []\n",
    "    for i in calle:\n",
    "        if i != ' ':\n",
    "            nombre_url.append(i)\n",
    "        else:\n",
    "            nombre_url.append('%20')\n",
    "    nombre_url.extend('%20')\n",
    "    \n",
    "    nombre_completo = ''.join([''+i for i in nombre_url])\n",
    "    \n",
    "    return nombre_completo+str(altura)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciamos el resultado de nuestra función, \n",
    "resultado = unifica_direccion(separa_partes(fachadas.direccion[0])['nombre'],\n",
    "                              separa_partes(fachadas.direccion[0])['altura'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y vemos qué nos devuelve. \n",
    "resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar en el ejemplo de [consulta](http://servicios.usig.buenosaires.gob.ar/normalizar/?direccion=juli%C3%A1n%20alvarez%202351,%20caba) que compartimos previamente, la dirección se encuentra codificada en un formato legible para la consulta que haremos a la url. Lo que hicimos acá es reemplazar los espacios por un `%20` para que nuestro string se acomode al encoding de ASCII. Ahora, vamos a proceder a hacer el request en sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el módulo request de urllib,\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciamos el string de la consulta sobre el que iremos cambiando el valor de la dirección\n",
    "url = 'http://servicios.usig.buenosaires.gob.ar/normalizar/?direccion={},%20caba'.format(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por ejemplo, con lo que guardamos en 'resultado'\n",
    "contents = urllib.request.urlopen(url).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esto es lo que nos devuelve\n",
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora convertimos el btype bytes en un diccionario para recolectar nuestros resultados\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para lo que cargamos nuestro resultado como json\n",
    "coords = json.loads(contents.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords['direccionesNormalizadas'][0]['coordenadas']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sí, modularizamos estos pasos en una funcion que nos devuelva las coordenadas. Lo que primero hará es adaptar el string de entrada al formato que usaremos en el url. Luego, lo introducirá en la consulta y, si esta fuese errónea pasará nuevamente el string por el normalizador. Este buscará una dirección en el texto que le pasamos para devolver otra normalizada que iniciará nuevamente el proceso de formateo para reintentar la consulta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devuelve_coordenadas(direccion):\n",
    "    '''\n",
    "    Procesa una dirección y realiza una consulta \n",
    "    web al normalizador USIG.\n",
    "    ...\n",
    "    Argumentos:\n",
    "        direccion(str): string de letras y numeros.\n",
    "    Devuelve:\n",
    "        dict: epsg, longitud y latitud de la direccion. \n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        # Contruye formato compatible con urlopen\n",
    "        tildes = {'á': 'a',\n",
    "                  'é': 'e',\n",
    "                  'í': 'i', \n",
    "                  'ó': 'o',\n",
    "                  'ú': 'u'}\n",
    "\n",
    "        for k in tildes.keys():\n",
    "            if k in direccion:\n",
    "                direccion = direccion.replace(k, tildes[k])\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        formato_web = unifica_direccion(separa_partes(direccion)['nombre'],\n",
    "                                        separa_partes(direccion)['altura'])\n",
    "\n",
    "        url = 'http://servicios.usig.buenosaires.gob.ar/normalizar/?direccion={},%20caba'\n",
    "        consulta = url.format(formato_web)\n",
    "\n",
    "        # Consulta el servicio de georreferencimiento y carga los resultados\n",
    "        print('Direccion ingresada: %s' % direccion)\n",
    "        resultado = urllib.request.urlopen(consulta).read()\n",
    "        carga_resultado = json.loads(resultado.decode('utf-8'))\n",
    "        print('Direccion conultada: %s' % direccion)\n",
    "\n",
    "\n",
    "        # Si la consulta falla, volvemos a pasar una direccion normalizada\n",
    "        if 'errorMessage' in carga_resultado.keys():\n",
    "            # Normalizamos la direccion para pasar un string legible\n",
    "            normalizar = normaliza(direccion, 'Texto')\n",
    "            direccion_normalizada = normalizar['nombre']+' '+str(normalizar['altura'])\n",
    "            print('Consultando direccion: %s' % direccion_normalizada)\n",
    "            nvo_formato_web = unifica_direccion(separa_partes(direccion_normalizada)['nombre'],\n",
    "                                                separa_partes(direccion_normalizada)['altura'])\n",
    "\n",
    "            nva_consulta = url.format(nvo_formato_web)\n",
    "            nvo_resultado = urllib.request.urlopen(nva_consulta).read()\n",
    "            nva_carga_resultado =json.loads(nvo_resultado.decode('utf-8'))\n",
    "            info = nva_carga_resultado['direccionesNormalizadas'][0]['coordenadas']\n",
    "        else:\n",
    "            info = carga_resultado['direccionesNormalizadas'][0]['coordenadas']\n",
    "\n",
    "        return info\n",
    "    except:\n",
    "        return 'Calle inexistente'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recordamos que fachadas.direccion[0] es\n",
    "fachadas.direccion[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos nuestra función\n",
    "devuelve_coordenadas('ALVAREZ, JULIAN 2531')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y ahora vemos qué hubiese devuelto si la dirección estaba escrita de otra manera\n",
    "devuelve_coordenadas('jUlián álVarez 2531 (esquina arenales)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos cuanto tarda en consultar mil casos\n",
    "inicio = time.time()\n",
    "coordenadas = fachadas.direccion[0:1000].map(devuelve_coordenadas)\n",
    "fin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiempo_total = (((fin - inicio)*26000)/1000)/60\n",
    "print('Tiempo de consulta para la totalidad de casos: %r minutos' % tiempo_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EL CAMINO CORTO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"525\" height=\"125\" src=\"imagenes/camino_corto.jpeg\" style=\"float: right; padding: 0 15px\">\n",
    "\n",
    "En lugar de adaptar manualmente los caracteres que están por fuera del encoding de la url, ahora vamos a apelar a una herramienta pensada específicamente para codificar una url. Esta es el módulo [parse de urllib](https://www.urlencoder.io/python/). Con el método `quote()` podemos acomodar cada uno de los caracteres de nuestras direcciones sin tener que estar atentos a cada uno de ellos específicamente. \n",
    "\n",
    "Esto nos permitirá ahorrar bastantes líneas de código y acortar considerablemente el cuerpo de nuestra función `devuelve_coordenadas`. Ya no buscaremos espacios o tildes, por ejemplo. Simplemente, pasaremos nuestro string por este método y obtendremos un resultado legible en ASCII encoding.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos parse\n",
    "import urllib.parse\n",
    "\n",
    "# y utilizamos el método quote para obtener un string en formato ascii\n",
    "urllib.parse.quote(fachadas.direccion[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos cómo el método quote nos permite acortar la función\n",
    "def devuelve_coordenadas(direccion):\n",
    "    '''\n",
    "    Procesa una dirección y realiza una consulta \n",
    "    web al normalizador USIG.\n",
    "    ...\n",
    "    Argumentos:\n",
    "        direccion(str): string de letras y numeros.\n",
    "    Devuelve:\n",
    "        dict: epsg, longitud y latitud de la direccion. \n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        # Contruye formato compatible con urlopen\n",
    "        url = 'http://servicios.usig.buenosaires.gob.ar/normalizar/?direccion={},%20caba'\n",
    "        direccion_ascii = urllib.parse.quote(direccion)\n",
    "        consulta = url.format(direccion_ascii)\n",
    "\n",
    "        # Consulta el servicio de georreferencimiento y carga los resultados\n",
    "        print('Direccion ingresada: %s' % direccion)\n",
    "        resultado = urllib.request.urlopen(consulta).read()\n",
    "        carga_resultado = json.loads(resultado.decode('utf-8'))\n",
    "        print('Direccion consultada: %s' % direccion)\n",
    "        info = carga_resultado['direccionesNormalizadas'][0]['coordenadas']\n",
    "        return info\n",
    "    except:\n",
    "        return 'Calle sin resultado'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutemos la función y veamos cuánto tiempo lleva\n",
    "inicio = time.time()\n",
    "coordenadas = fachadas.direccion[0:1000].map(devuelve_coordenadas)\n",
    "fin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiempo_total = (((fin - inicio)*26000)/1000)/60\n",
    "print('Tiempo de consulta para la totalidad de casos: %r minutos' % tiempo_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluando uno contra otro, no es que el camino corto nos permitirá ejecutar nuestra función de una manera más rápida. Las dos tardan aproximadamente lo mismo. Algo que era esperable, porque ambas deben hacer una consulta (lo único en lo que difieren es la manera en la que codifican el string). Cuestión que no es menor, ya que apelando al método `quote` no sólo conseguimos que nuestra función ahorre líneas de código. También nos asegura no estar sujeto a posibles excepciones (caracteres que no conocemos en nuestra serie de direcciones dada su extensión). Este método estandariza lo que haga falta de manera general sin tener que estar atento caracter por caracter, lo que indudablemente hace que nuestra función sea más consistente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapeamos nustra función a toda la serie de direcciones\n",
    "coordenadas = fachadas.direccion.map(devuelve_coordenadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos devolvió al menos algún valor para todos nuestros casos. La extensión es igual al de la serie de direcciones.\n",
    "len(coordenadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por cada row nos devuelve un diccionario que debemos transformar en columnas.\n",
    "pd.Series(coordenadas)[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos una copia de nuestro dataframe de fachadas\n",
    "fachadas_ = fachadas.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accedemos a la key srid de cada item de nuestra serie de direcciones con una lista por comprension\n",
    "epsg = [coordenadas[i]['srid'] if coordenadas[i] != 'Calle sin resultado' else \n",
    "        'Sin dato' for i in range(len(coordenadas))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El resultado es el que esperabamos, tenemos la misma cantidad de casos.\n",
    "len(epsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos lo mismo con la longitud\n",
    "lon = [coordenadas[i]['x'] if coordenadas[i] != 'Calle sin resultado' else \n",
    "        'Sin dato' for i in range(len(coordenadas))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y la latitud.\n",
    "lat = [coordenadas[i]['y'] if coordenadas[i] != 'Calle sin resultado' else \n",
    "        'Sin dato' for i in range(len(coordenadas))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y guardamos todo como series de nuestro df de fachadas\n",
    "fachadas_['lat'], fachadas_['lon'], fachadas_['epsg'] = lat, lon, epsg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fachadas_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportemoslo para no tener que correr todo de nuevo\n",
    "#fachadas_.to_csv('data/fachadas_usig.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('data/fachadas_usig.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPARACION DE RESULTADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fachadas_sndg = pd.read_csv('data/fachadas_sndg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fachadas_sndg.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fachadas_usig = pd.read_csv('data/fachadas_usig.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fachadas_usig.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construye_coords(x,y):\n",
    "    try:\n",
    "        geometria = Point(float(x), float(y)) \n",
    "        return geometria\n",
    "    except:\n",
    "        return 'sin coordenadas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construye_gdf(df,proj):\n",
    "    \n",
    "    geom = df.apply(lambda x: construye_coords(x.lon, x.lat), axis=1)\n",
    "    df['geometry'] = geom\n",
    "    localizables = df.loc[df['geometry']!='sin coordenadas']\n",
    "    \n",
    "    if type(proj) == str:\n",
    "        crs = pyproj.CRS.from_user_input(proj)\n",
    "        \n",
    "    else:\n",
    "        crs = pyproj.CRS(proj)\n",
    "    \n",
    "    gdf = gpd.GeoDataFrame(localizables, crs=crs, geometry=localizables.geometry)\n",
    "    print('Devolviendo geodataframe con {} casos perdidos'.format(len(df)-len(gdf)))\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wkt = \"\"\"PROJCS[\"GKBA\",\n",
    "        GEOGCS[\"International 1909 (Hayford)\",\n",
    "            DATUM[\"CAI\",\n",
    "                SPHEROID[\"intl\",6378388,297]],\n",
    "            PRIMEM[\"Greenwich\",0],\n",
    "            UNIT[\"degree\",0.0174532925199433]],\n",
    "        PROJECTION[\"Transverse_Mercator\"],\n",
    "        PARAMETER[\"latitude_of_origin\",-34.6297166],\n",
    "        PARAMETER[\"central_meridian\",-58.4627],\n",
    "        PARAMETER[\"scale_factor\",0.999998],\n",
    "        PARAMETER[\"false_easting\",100000],\n",
    "        PARAMETER[\"false_northing\",100000],\n",
    "        UNIT[\"Meter\",1]]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_usig = construye_gdf(fachadas_usig,wkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_sndg = construye_gdf(fachadas_sndg,wkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que finalmente tenemos nuestros geodataframes de puntos listos veamos si las dos herramientas nos devuelven los mismos resultados. Para eso, repasemos algunos conceptos básicos de visualzación. Cuando trabajamos con algunas librerías como matplotlib, es necesario tanto la figura donde se van a plotear nuestros gráficos como el lugar que va a ocupar. En líneas generales, lo que nos debe quedar en claro es que la instancia que nosotros creemos para la figura será siempre el método `figure` del módulo `pyplot` de `matplotlib`. Esto nos permitirá definir atributos como el tamaño. Para la posición de la figura, se utiliza el método `add_subplot` (aplicable siempre a un objeto de tipo `pyplot`. Así, se define la forma en la que se disponen las figuras. En nuestro caso, si queremos que el primer mapa se disponga al lado del segundo o, si por ejemplo, queremos superponerlos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos la figura con un tamaño específico\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "# disponemos dos ejes en una columna. El tercero que pasamos como parametro es el número de eje\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "# acá vemos el eje #2\n",
    "ax2 = fig.add_subplot(1,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "gdf_usig.plot(ax=ax1, color='red')\n",
    "gdf_sndg.plot(ax=ax2, color='blue')\n",
    "\n",
    "ax1.set_axis_off()\n",
    "ax1.set_title('Resultados Normalizador USIG')\n",
    "ax2.set_axis_off()\n",
    "ax2.set_title('Resultados Normalizador SNDG');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si quisieramos verlos superpuestos\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "gdf_usig.plot(ax=ax, color='red')\n",
    "gdf_sndg.plot(ax=ax, color='blue')\n",
    "ax.set_axis_off()\n",
    "ax.set_title('Resultados USIG/SNDG');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar, los resultados son bastante similares. Con lo cual, a nivel performance ambos normalizadores trabajaron de manera muy parecida. Pero dónde es que caen esos puntos realmente? En los plots anteriores se puede ver con claridad la forma de la ciudad. De todos modos, es muy común cuando trabajamos con este tipo de situaciones en las que ploteamos algo cuyo resultado inicial nos resulta incierto, querer ver nuestros datos con una especificidad mayor. Hacer zoom, ver en qué polígono cae un punto, el nombre de una calle, etc. Para esto, existen varias librerías que permiten, de una manera muy ágil, plotear figuras sobre un layer dinámico (como si fuese un mapa web). A continuación daremos un pantallazo de algunas de ellas..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **1. Leaflet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos la librería\n",
    "import mplleaflet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instanciamos el plot\n",
    "ax = gdf_usig.head(1000).plot(markersize = 50, color = \"red\")\n",
    "\n",
    "# Lo visualiamos con el método display\n",
    "mplleaflet.display(fig=ax.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si prestamos atención al warning, se nos sugiere utilizar `display` de Ipython. Para ello, vamos a tener que, primero, exportar nuestro mapa y traer la ruta a ese `ħtml` que exportamos. Con leaflet también podemos hacer esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploteamos la data,\n",
    "gdf_usig.head(1000).plot(markersize = 50, color = \"red\");\n",
    "# la exportamos\n",
    "#mplleaflet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y la llamamos nuevamente. IFrame nos permite regular el alto y el ancho del output\n",
    "path = '_map.html'\n",
    "from IPython.display import IFrame    \n",
    "IFrame(path, width=1000, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **2. Contextily**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra es Contextily, una librería muy útil para plotear puntos, polígonos o líneas sobre un layer estático que nos de un primer pantallazo del mapa que buscamos construir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos la librería\n",
    "import contextily as ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puntos = gdf_usig.plot(figsize=(10, 10), alpha=0.5, color='red', edgecolor='k')\n",
    "ctx.add_basemap(puntos,zoom=12, crs=4326)\n",
    "puntos.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerdan que en la clase anterior, vimos que en la nueva release de `Geopandas` el sistema de proyección de coordenadas (o CRS) había incorporado una mejora en su manera de definirlo? Y recordemos también que, el motivo de este cambio se apoya en la idea de sustitur un `proj4string` (o simplemente un string con información sobre el sistema de proyección utilizado) por un nuevo objeto enriquecido que nos permite acceder a distintos tipos de atributos y métodos. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esta es la información de nuestro crs\n",
    "gdf_usig.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con la que podemos acceder, por ejemplo al nombre\n",
    "gdf_usig.crs.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o al datum\n",
    "gdf_usig.crs.datum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, esta nueva forma de definir el objeto CRS puede tener un impacto importante en el uso de distintas librerías. Y seguramente llevará un tiempo hasta que esto termine siendo totalmente compatible o todas se adapten al uso de este nuevo objecto. El principal causal de esto que existen muchas fuentes y formas de definir un CRS, algunas de las cuales pueden tener una descripción que no se ajusta completamente a los nuevos estándares de PROJ> 6 (cadenas de proj4, formatos WKT más antiguos, etc.). Como se menciona en la documentación oficial de geopandas sobre el [uso de proyecciones](https://geopandas.org/projections.html), en estos casos, el objeto pyproj.CRS que se obteiene, podría contener algunas incosistencias. Por ejemplo, un código EPSG que no se condice con la localización esperada.En el [blog](https://jorisvandenbossche.github.io/blog/2020/02/11/geopandas-pyproj-crs/) de [Joris Van der Boschee](https://jorisvandenbossche.github.io/pages/about.html) se brindan algunas explicaciones adicionales bastante interesantes. \n",
    "\n",
    "Pero por qué mencionamos esto ahora? Si prestan atención, en la primera instancia del mapa que construimos con `Contextily` el crs que definimos fue el clásico `4326`. ¿Por qué hicimos esto si nosotros construimos el gdf a partir de un `wkt`? Veamos qué hubiese pasado si definíamos el `CRS` de otra manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por ejemplo, si hubiésemos querido asignar el crs de otro dataframe cuyo epsg nos es familiar\n",
    "barrios = gpd.read_file('carto/barrios_badata.shp')\n",
    "barrios.crs.datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero vemos que nos devuelve un error relativo al crs que definimos desde el wkt\n",
    "puntos = gdf_usig.to_crs(barrios.crs).plot(figsize=(10, 10), alpha=0.5, color='red', edgecolor='k')\n",
    "ctx.add_basemap(puntos,zoom=12, crs=barrios.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuestro gdf pero ahora no intentando reproyectar sino asigando el CRS deseado directamente\n",
    "gdf_usig_crs_barrios = construye_gdf(fachadas_usig,barrios.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y volvemos a intentar la operación\n",
    "puntos = gdf_usig_crs_barrios.plot(figsize=(10, 10), alpha=0.5, color='red', edgecolor='k')\n",
    "ctx.add_basemap(puntos,zoom=12, crs=gdf_usig_crs_barrios.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Contextily` sigue sin reconocer nuestro sistema de coordenadas. Intentemos un camino diferente..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos en la descripción de nuestro objeto CRS cual es el código EPSG asignado\n",
    "gdf_usig_crs_barrios.crs.datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y intentemos nuevamente...\n",
    "puntos = gdf_usig_crs_barrios.plot(figsize=(10, 10), alpha=0.5, color='red', edgecolor='k')\n",
    "ctx.add_basemap(puntos,zoom=12, crs=6221)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pueden intentarlo tanto con este código como con el que vimos inicialmente en la descripción de nuestro primer CRS (9001). Ambos devolverán un mensaje de error de `CRS no soportado`. Esto, porque `Contextily` no está reconociendo la definición de ninguno de los dos (el que heredamos de nuestro shape de barrios o el especificado en el wkt). \n",
    "\n",
    "Y qué hubiese pasado si apelábamos al [EPSG oficial](https://ramsac.ign.gob.ar/posgar07_pg_web/documentos/Informe_sobre_codigos_oficiales_EPSG.pdf) que corresponde a la faja donde cae la Ciudad de Buenos Aires? Pruébenlo y cuéntennos cómo les va!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manera diferente, esto no nos hubiese pasado, por ejemplo, con `leaflet`. Como dijimos antes, el impacto que tiene este nuevo objeto no es siempre el mismo. Veamoslo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciamos el plot con nuestro nuevo gdf\n",
    "ax = gdf_usig_crs_barrios.head(1000).plot(markersize = 50, color = \"red\")\n",
    "\n",
    "# Y lo visualiamos con el método display\n",
    "mplleaflet.display(fig=ax.figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el gdf con el EPSG que vinos inicialmente en datum\n",
    "gdf_usig_9001 = construye_gdf(fachadas_usig,9001)\n",
    "\n",
    "# Y repetimos la operatioria\n",
    "ax = gdf_usig_9001.head(1000).plot(markersize = 50, color = \"red\")\n",
    "\n",
    "mplleaflet.display(fig=ax.figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este sería el resultado con Contextily si no se especificara el CRS\n",
    "puntos = gdf_usig_crs_barrios.plot(figsize=(10, 10), alpha=0.5, color='red', edgecolor='k')\n",
    "ctx.add_basemap(puntos,zoom=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tipo de circunstancias, algo que es muy recomendable es trabajar directamente en `4326` (para asegurarnos que no tendremos ningún problema de incompatibilidad o falta de soporte). O bien, este nuevo objecto `pyproj.CRS` cuenta con un método `to_epsg()` para devolver un id equivalente. Este es bastante útil para identificar si nuestra proyección (en nuestro caso definida desde un wkt) está siendo identificada correctamente..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En nuestro caso, vemos que el epsg es un None type, algo que ya nos debería parecer sospechoso\n",
    "print(gdf_usig.crs.to_epsg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# También podemos acceder a la descripción original si hacemos:\n",
    "gdf_usig.crs.source_crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y veamos también que ahora trabajamos con un nuevo parámetro: min_confidence\n",
    "gdf_usig.crs.source_crs.to_epsg(min_confidence=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto se encuentra bastante bien explicado en la documentación de [pyproj](https://pyproj4.github.io/pyproj/stable/gotchas.html#what-is-the-best-format-to-store-the-crs-information). Si el método `to_epsg` no nos devuelve ningún código EPSG que coincida con el wkt o el pyproj4 string que usamos inicialmente (porque no lo encuentra o porque no existe), podemos apelar al parámetro `min_confidence`. Este, nos permitiría obtener el código EPSG más cercano, alternando los umbrales que estamos dispuestos a tolerar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos que ahora Contextily interpeta el crs...\n",
    "puntos = gdf_usig.plot(figsize=(10, 10), alpha=0.5, color='red', edgecolor='k')\n",
    "ctx.add_basemap(puntos,zoom=12, crs=gdf_usig.crs.source_crs.to_epsg(min_confidence=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, si bien el resultado es bastante similar al que obtuvimos con un EPSG igual a 4326, este método nos puede resultar de utilidad para situaciones en las que necesitemos trabajar con una proyección diferente asegurándonos que el mismo sea interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **3. Folium**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folium es una librería que combina distintos tipos de funcionalidades. Desde controles de zoom y la posibilidad de alternar layers hasta clases que devuelven mapas de calor y coropletas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si no lo incluiste en los requirements, lo podemos instalar directo desde el notebook\n",
    "#!pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la librería\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un nuevo gdf en 4326 para los resultados de ambos nomralizadores\n",
    "sndg_4326 = construye_gdf(fachadas_sndg,4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usig_4326 = construye_gdf(fachadas_usig,4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos nuestro gdf de puntos en geojson\n",
    "sndg_gjson = folium.features.GeoJson(sndg_4326.head(1000), name=\"SNDG\")\n",
    "\n",
    "# Acá podemos ver la estructura\n",
    "#points_gjson.data.get('features')\n",
    "\n",
    "# Y ploteamos el mapa...\n",
    "mapa = folium.Map(location=[-34.6157437, -58.4333812], \n",
    "               zoom_start=11, \n",
    "               control_scale=True)\n",
    "sndg_gjson.add_to(mapa);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapa_folium(gdf, polygons, barrio, exporta):\n",
    "    '''\n",
    "    Construye un mapa de puntos y/o polígonos sobre\n",
    "    un layer dinámico de leaflet.\n",
    "    ...\n",
    "    Argumentos:\n",
    "     gdf(GeoDataFrame): Geodataframe de puntos.\n",
    "     barrio (str): nombre del barrio.\n",
    "     polygons (GeoDataFrame): GeoDataFrame de polígonos.\n",
    "     exporta (bool): True para exportar.\n",
    "     \n",
    "    Devuelve:\n",
    "      folium.Map : mapa de puntos/polígonos  \n",
    "    '''\n",
    "    \n",
    "    # Creamos el layer de leaflet. Lo centramos en la Ciudad de Buenos Aires\n",
    "    centroide =  gdf.geometry.centroid\n",
    "    coordenadas = [centroide.y.mean(), centroide.x.mean()]\n",
    "    layer = folium.Map(location=coordenadas, \n",
    "                       zoom_start=11, \n",
    "                       height = 500,\n",
    "                       control_scale=True)\n",
    "    \n",
    "    \n",
    "    # Si pasamos polígonos de base, también los cargamos al mapa\n",
    "    if polygons is not None:\n",
    "        estilo = lambda x: {'fillColor': 'red' if\n",
    "                            x['properties']['BARRIO']== barrio else\n",
    "                            'grey'}\n",
    "        \n",
    "        pol_hover= folium.features.GeoJsonTooltip(\n",
    "                        fields=['BARRIO','COMUNA'],\n",
    "                        aliases=['Barrio:','Comuna:'])\n",
    "        \n",
    "        folium.GeoJson(polygons.to_crs(gdf.crs),\n",
    "                       name = 'Barrios de la ciudad',\n",
    "                       style_function = estilo,\n",
    "                       tooltip = pol_hover\n",
    "                       ).add_to(layer)\n",
    "\n",
    "    \n",
    "    # Agregamos nuestros puntos con el método CircleMarker\n",
    "    for i, row in gdf.iterrows():\n",
    "        folium.CircleMarker((row.lat,row.lon),            \n",
    "                            radius=3, weight=2, \n",
    "                            color='blue', \n",
    "                            fill_color='blue', \n",
    "                            fill_opacity=.5,\n",
    "                            tooltip= 'Certificado:'+row.vencimiento_certificado).add_to(layer)\n",
    "    \n",
    "    \n",
    "    # Exportar el resultado\n",
    "    if exporta:\n",
    "        layer.save('mapa.html')\n",
    "    \n",
    "    folium.LayerControl(autoZIndex=False, collapsed=False).add_to(layer)\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploteamos un mapa sólo de puntos\n",
    "mapa_folium(sndg_4326.head(50), None, None, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora lo combinamos con un gdf de polígonos\n",
    "mapa_folium(sndg_4326.head(1000), barrios, 'VILLA LUGANO', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "barrios.plot(ax=ax1, color='grey')\n",
    "usig_4326.to_crs(barrios.crs).plot(ax=ax1, color='red', markersize=1)\n",
    "\n",
    "barrios.plot(ax=ax2, color='grey')\n",
    "sndg_4326.to_crs(barrios.crs).plot(ax=ax2, color='blue', markersize=1)\n",
    "\n",
    "\n",
    "ax1.set_axis_off()\n",
    "ax2.set_axis_off()\n",
    "\n",
    "ax1.set_title('Resultados Normalizador USIG')\n",
    "ax2.set_title('Resultados Normalizador SNDG');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "barrios.plot(ax=ax1, color='grey')\n",
    "usig_4326.to_crs(barrios.crs).plot(ax=ax1, color='red', markersize=10, edgecolor='white')\n",
    "\n",
    "barrios.plot(ax=ax2, color='grey')\n",
    "sndg_4326.to_crs(barrios.crs).plot(ax=ax2, color='blue', markersize=10, edgecolor='white')\n",
    "\n",
    "\n",
    "ax1.set_axis_off()\n",
    "ax2.set_axis_off()\n",
    "\n",
    "ax1.set_title('Resultados Normalizador USIG')\n",
    "ax2.set_title('Resultados Normalizador SNDG');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapa_de_calor(gdf, radio, inicio, fin):\n",
    "    '''\n",
    "    Construye un mapa de calor sobre\n",
    "    un layer dinámico de leaflet.\n",
    "    ...\n",
    "    Argumentos:\n",
    "     gdf(GeoDataFrame): Geodataframe de puntos.\n",
    "     radio (int): radio de cada punto.\n",
    "     inicio (int): límite inferior.\n",
    "     fin (int): límite superior.\n",
    "     \n",
    "    Devuelve:\n",
    "      folium.Map : mapa de calor  \n",
    "    '''\n",
    "    \n",
    "    # Ubicamos el centro del mapa\n",
    "    centroide =  gdf.geometry.centroid\n",
    "    coordenadas = [centroide.y.mean(), centroide.x.mean()]\n",
    "    \n",
    "    # Conseguimos la latitud y la longitud de cada punto con una función anónima\n",
    "    x = gdf[\"geometry\"].apply(lambda punto: punto.x)\n",
    "    y = gdf[\"geometry\"].apply(lambda punto: punto.y)\n",
    "\n",
    "    # Convertimos esas coordenadas en una lista de tuplas\n",
    "    puntos = list(zip(y, x))\n",
    "    \n",
    "    # Creamos el layer de leaflet\n",
    "    layer = folium.Map(location=coordenadas, \n",
    "                       zoom_start=11, control_scale=True)\n",
    "\n",
    "    # Agregamos el mapa de calor al layer que habíamos instanciado\n",
    "    HeatMap(puntos[inicio:fin], \n",
    "            radius=radio).add_to(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# también se puede jugar con otros parametros como max_zoom o el tile para elegir otro base map\n",
    "mapa_de_calor(usig_4326, 15, 1000, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para concluir con esta práctica, veamos qué se puede hacer con los datasets que hemos venido construyendo. Un aspecto muy importante en este tipo de situaciones en las que la construcción de nuestros set de datos se vuelve un proceso lento y demandante, es no olvidar o dejar de lado la pregunta que nos va a ayudar a responder. Y no por la pregunta en sí, ya que puede ser una o más. Sino porque, la comunicación de nuestros hallazgos se vuelve mucho más clara cuando respondemos algo en concreto. Por ejemplo, dónde se aglomera nuestra variable de interés? existe algún patrón de localización específico?\n",
    "\n",
    "Con nuestro mapa de calor, ya pudimos identificar algunas posibles zonas. Sin embargo, vamos a aprovechar esta pregunta para introducir una nueva geometría, un tipo de polígono que no vimos hasta ahora. Este no es nada más y nada menos que el radio censal. Una división o unidad geográfica que utiliza el [Instituto Nacional de Estadísticas y Censos (INDEC)](https://www.indec.gob.ar/indec/web/Institucional-Indec-Codgeo) para sus relevamientos censales. \n",
    "\n",
    "Pero no vamos a ahondar mucho en este tema. Simplemente, mencionaremos que una de sus principales ventajas, es la posibilidad de contar con información sociodemográfica a un alto nivel de desagregación territorial (algo que se vuelve importante en términos de acceso a información útil para describir con detalle áreas más reducidas - o con mayor zoom). Tema que retomaremos en otro notebook.\n",
    "\n",
    "A continuación, veremos un ejemplo en el que utilizaremos los radios solamente por el tamaño de los polígonos (también existen otras librerías muy útiles para lograr este tipo de efectos en nuestros outputs). Haremos un join espacial para ver la cantidad de puntos (nuestras fachadas registradas en las partidas de conservación patrimonial) que caen dentro de nuestros polígonos censales y evaluaremos la concentración de actividad (personas que hayan registrado una fachada) en base a la clasificación de este valor dentro de los radios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos en la clase anterior, `Geopandas` cuenta con un módulo bastante útil que nos permite detectar la relación espacial entre dos geometrías. Este se denomina [spatial join](https://geopandas.org/mergingdata.html#spatial-joins) y nos permitira conocer cuántos puntos caen dentro del área de cada polígono."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos el módulo desde gepandas\n",
    "from geopandas import sjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos el shapefile de radios censales\n",
    "radios = gpd.read_file('carto/informacion_censal_por_radio_2010.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergeamos ambos gdf\n",
    "fachadas_r = gpd.sjoin(sndg_4326, radios.to_crs(sndg_4326.crs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizamos el reultado...\n",
    "fachadas_r.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupamos la cantidad de fachadas con certificado de conservación por radio censal\n",
    "fachadas_por_radio = fachadas_r.groupby(['RADIO_I'])[['partida']].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un diccioanrio donde la key es el id del radio y el value la cantidad de partidas.\n",
    "d = dict(zip(fachadas_por_radio['RADIO_I'], fachadas_por_radio['partida']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos esta nueva información como columna adicional de nuestro shape original de radios.\n",
    "radios['partidas'] = radios['RADIO_I'].map(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos una primera visualización de este resultado.\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "radios.to_crs(wkt).plot(ax=ax1, column='partidas', scheme='Natural_Breaks', k=3, \n",
    "                        linewidth=0.2, edgecolor='black', cmap='Reds_r', legend=True)\n",
    "radios.to_crs(wkt).plot(ax=ax2, column='partidas', scheme='Quantiles', k=3, \n",
    "                        linewidth=0.2, edgecolor='black', cmap='Blues_r', legend=True)\n",
    "\n",
    "ax1.set_axis_off()\n",
    "ax2.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a introducir otro tema que retomaremos en el notebook siguiente: esquemas de clasificación. Acá simplemente vamos a comparar dos distintos para detectar si existe algún tipo de diferencia. Como no se pueden apreciar contrastes importantes sólo nos detendremos brevemente en la última distribución. Los `quantiles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reemplazamos los casos sin valor por 0 (en nuestro join espacial muchos polígonos no coincidían con ningún punto)\n",
    "radios['partidas'].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculamos nuestros bins de corte (quantiles). \n",
    "qcut = list(radios['partidas'].quantile([0, 0.25, 0.5, 0.75, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto nos devuelve cinco intervalos, o quintiles que dividen a nuestra población en cinco partes iguales\n",
    "pd.qcut(radios['partidas'], 5, labels=False).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qué significa esto? Básicamente que el primer 20% de nuestros radios censales no tendrá fachadas o partidas dentro del área delimitada por los bordes del polígono. El segundo 20%, es decir entre el 20 y el 40% de los casos tendrá un máximo de 2 partidas. Del 40 al 60 será entre 2 y 5, del 60 al 80% entre 5 y 10 y el último quintil (o 20% de nuestros casos) trendrá entre 10 y 75 partidas. Es decir, el más disperso. Pero como dijimos, este tema lo retomaremos en nuestra próxima clase cuando revisemos algunas clases y métodos de la librería `mapclassify`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuestros bins de corte\n",
    "qcut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a utilizar la clase `Choropleth` de `Folium` para plotear nuestro agrupamiento por radio. Presten atención a la lógica general. Esta no difiere de los otros mapas que hemos visto. Primero agregar el mapa con `.Map()`(para lo que dejamos seteado un camino que siempre nos devuelva el centro de nuestro gdf a partir de los ceontroides de las latitudes y longitudes). Y luego, ir creando los distintos objetos que iremos agregando como capas superpuestas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos el mapa\n",
    "gdf = radios.copy()\n",
    "centroide =  gdf.geometry.centroid\n",
    "coordenadas = [centroide.y.mean(), centroide.x.mean()]\n",
    "layer = folium.Map(location=coordenadas, \n",
    "                   zoom_start=11, \n",
    "                   height = 500,\n",
    "                   control_scale=True)\n",
    "\n",
    "# esto nos permitiría seleccionar entre distintos tiles de base\n",
    "tiles = ['stamenwatercolor', 'cartodbpositron', 'openstreetmap', 'stamenterrain']\n",
    "for tile in tiles:\n",
    "    folium.TileLayer(tile).add_to(layer)\n",
    "\n",
    "# instanciamos la coropleta\n",
    "fachadas = folium.Choropleth(name='Radios censales',\n",
    "                             geo_data=gdf,\n",
    "                             data=gdf[[c for c in gdf.columns if c!='geometry']],\n",
    "                             columns=['RADIO_I','partidas'],\n",
    "                             key_on='properties.RADIO_I',\n",
    "                             fill_color='Reds_r', \n",
    "                             bins = qcut,\n",
    "                             fill_opacity=0.6, \n",
    "                             line_opacity=0.2,\n",
    "                             highlight=True,\n",
    "                             legend_name='Cantidad de partidas por radio censal (CNPHV-2010)',\n",
    "                             smooth_factor=1).add_to(layer)\n",
    "\n",
    "# creamos un tooltip para hacer un hover en nuestros polígonos\n",
    "pol_hover = folium.features.GeoJsonTooltip(fields=['BARRIO','RADIO_I','partidas'],\n",
    "                                           aliases=['Barrio:','Radio_id:','Partidas:'])\n",
    "\n",
    "# Y lo agregamos al layer inicial\n",
    "folium.GeoJson(gdf,\n",
    "               style_function=lambda x: {\"weight\":0.35, \n",
    "                                         'color':'white', \n",
    "                                         'fillOpacity':0.0},\n",
    "               smooth_factor=2.0,\n",
    "               tooltip=pol_hover).add_to(fachadas);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante remarcar que, en este ejemplo, hemos pasado un geodataframe a la clase `Choropleth` de folium. También podríamos haberle pasado directamente un objeto de tipo `GeoJson`, que es lo que la clase usa para mapear nuestros polígonos (buscar la key, aplicar estilos, etc.). De hecho, si se fijan en el parámetro `key_on`, se accede a la columna de radios a través de la key `properties`. Veamos cómo es eso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos un geojson de radios censales (es nuestro mismo gdf que previamente exportamos en este formato)\n",
    "with open('radios.geojson') as f:\n",
    "    rg = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos las keys...\n",
    "rg.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y adentro de features, nuestras columnas!\n",
    "rg['features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este primer resultado nos permite ver dos cosas. Primero, una división fuerte entre dos grupos. Si bien nuestro último quintil tiene un amplio rango entre la base y el techo del intervalo, nos sirve para diferenciar dos áreas de la Ciudad. aquellas con nula o casi nula actividad y aquellas donde los radios tienen un número de certificados de conservación superior al techo del resto. \n",
    "\n",
    "En otras palabras, los radios censales con más de diez fachadas registradas se concentran desde el centro hacia el oeste, siguiendo la traza de la Avenida Rivadavia hasta poco más de Primera Junta. Y del centro hacia el Norte. Aprovechemos para agregar alguna traza que nos permita ver este efecto con mayor claridad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciamos el callejero de la Ciudad, que también bajamos de Buenos Aires Data.\n",
    "callejero = gpd.read_file('carto/callejero.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos las avenidas\n",
    "avenidas = callejero.loc[callejero['tipo_c']=='AVENIDA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos quedamos con algunas trazas que vayan del centro al oeste y del centro al norte\n",
    "ppales = avenidas.loc[(avenidas['nom_mapa']=='AV. DE MAYO')|(avenidas['nom_mapa']=='AV. RIVADAVIA')|\n",
    "                     (avenidas['nom_mapa']=='AV. CORRIENTES')|(avenidas['nom_mapa']=='AV. SANTA FE')|\n",
    "                     (avenidas['nom_mapa']=='AV.CABILDO')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cortamos rivadavia y eliminamos algunas secciones con altura cero\n",
    "pp_clean = ppales.loc[(ppales['nom_mapa']=='AV. DE MAYO')|\n",
    "                      ((ppales['nom_mapa']=='AV. RIVADAVIA')&\n",
    "                      (ppales['alt_izqfin']<8000)&(ppales['alt_derfin']<8000)&\n",
    "                      # tramos de rivadavia con altura = 0\n",
    "                      (ppales['id']!=19894)&(ppales['id']!=19844)&(ppales['id']!=19104)&(ppales['id']!=31505))|\n",
    "                      (ppales['nom_mapa']=='AV. CORRIENTES')|(ppales['nom_mapa']=='AV. SANTA FE')|\n",
    "                      (ppales['nom_mapa']=='AV.CABILDO')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volvemos a plotear nuestra coropleta. \n",
    "gdf = radios.copy()\n",
    "centroide =  gdf.geometry.centroid\n",
    "coordenadas = [centroide.y.mean(), centroide.x.mean()]\n",
    "layer = folium.Map(location=coordenadas, \n",
    "                   zoom_start=11, \n",
    "                   height = 500,\n",
    "                   control_scale=True)\n",
    "\n",
    "tiles = ['stamenwatercolor', 'cartodbpositron', 'openstreetmap', 'stamenterrain']\n",
    "for tile in tiles:\n",
    "    folium.TileLayer(tile).add_to(layer)\n",
    "\n",
    "\n",
    "fachadas = folium.Choropleth(name='Radios censales',\n",
    "                             geo_data=gdf,\n",
    "                             data=gdf[[c for c in gdf.columns if c!='geometry']],\n",
    "                             columns=['RADIO_I','partidas'],\n",
    "                             key_on='properties.RADIO_I',\n",
    "                             fill_color='Reds_r', \n",
    "                             bins = qcut,\n",
    "                             fill_opacity=0.6, \n",
    "                             line_opacity=0.2,\n",
    "                             highlight=True,\n",
    "                             legend_name='Cantidad de partidas por radio censal (CNPHV-2010)',\n",
    "                             smooth_factor=1).add_to(layer)\n",
    "\n",
    "pol_hover = folium.features.GeoJsonTooltip(fields=['BARRIO','RADIO_I','partidas'],\n",
    "                                           aliases=['Barrio:','Radio_id:','Partidas:'])\n",
    "\n",
    "folium.GeoJson(gdf,\n",
    "               style_function=lambda x: {\"weight\":0.35, \n",
    "                                         'color':'white', \n",
    "                                         'fillOpacity':0.0},\n",
    "               smooth_factor=2.0,\n",
    "               tooltip=pol_hover).add_to(fachadas)\n",
    "\n",
    "\n",
    "# ...pero ahora agregando un layer de calles.\n",
    "folium.GeoJson(pp_clean,\n",
    "               name='Avenidas',\n",
    "               style_function=lambda x: {'weight':2,'color':'#355C7D'},\n",
    "               highlight_function=lambda x: {'weight':5,'color':'yellow'},\n",
    "               tooltip=folium.features.GeoJsonTooltip(fields=['nom_mapa','alt_izqfin','alt_derfin'],\n",
    "                                                      aliases=['Calle:','Altura(izq):','Altura(der):']),\n",
    "               smooth_factor=5.0,\n",
    "              ).add_to(layer)\n",
    "\n",
    "# También agregamos un selector de layers\n",
    "folium.LayerControl().add_to(layer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sí, nuestra choropleta está lista. Qué podemos ver? Inicialmente que la concentración más alta queda denotada por el mismo modelo territorial de la ciudad. Del centro al oeste y del centro al norte (lo que se puede ver a través de las trazas de las avenidas que seleccionamos). Como dijimos anteriormente, si bien el rango es alto (de 10 a 75), nos permite ver una clara diferencia con el resto de la ciudad. Allí es donde se concentran las fachadas que han sido patrimoniadas. Un segundo paso podría ser filtrar este universo y evaluar dónde es que vuelven a concentrarse. Pero ese análisis se lo dejamos a ustedes!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-data",
   "language": "python",
   "name": "python-data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
